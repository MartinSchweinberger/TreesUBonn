--- 
title: "An introduction to conditional inference trees in R"
author: "Martin Schweinberger"
date: "Jan. 19, 2023"
site: bookdown::bookdown_site
documentclass: book
bibliography: ["book.bib", "packages.bib"]
biblio-style: apalike
link-citations: yes
---

# Introduction

This workshop focuses on conditional inference trees and their implementation in R.

```{r martin, echo=FALSE, out.width= "30%", out.extra='style="float:right; padding:10px"'}
knitr::include_graphics("https://slcladal.github.io/images/martinsface.jpg")
```

## About me

Quantitative corpus-linguist  

* Lecturer in Applied linguistics at the University of Queensland (UQ)  
* Associate Professor II at the Arctic University of Norway in Troms√∏ (UiT)
  
* Co-Director of the Language Technology and Data Analysis Laboratory (LADAL) at UQ
  
* Principal Data Science Advisor to the AcqVA Aurora Lab at UiT
  
* Projects: 
  * Australian Text Analytics Platform (ATAP)  
  * Language Data Commons of Australia (LDaCA)  
  
Studied Philosophy, English Philology and Psychology in Kassel and Galway
PhD in Hamburg, Post docs (or similar positions) at the Linguistic Diversity in Urban Areas Excellence Cluster (LiMA), the FU Berlin, the Universities of Greifswald, Luneburg, and Kassel

## About the workshop

Timeline | Table of Contents
* Introduction  
* When to use trees 
* What are pros and cons?    
* Case study | Practice    
* Outro  

During practice, we will use a Jupyter notebook and you can   
* sit back and follow   
* you can practice using the data provided by me  
* you can try and use your own data (but I cannot help you in modifying the code)




<div class="warning" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
[![Colab](https://slcladal.github.io/images/colab_lg.png)](https://colab.research.google.com/drive/1yi0hwcwfl5k01XfmpkEgpOf1jL1ObKcC?usp=sharing)<br>
[**Click this link to open an interactive version of this tutorial on Google Colab**](https://colab.research.google.com/drive/1yi0hwcwfl5k01XfmpkEgpOf1jL1ObKcC?usp=sharing). <br> This interactive Jupyter notebook allows you to execute code yourself and you can also change and edit the notebook, e.g. you can change code and upload your own data. <br>
</p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>

Tree-structure models  fall into the machine-learning rather than the inference statistics category as they are commonly used for classification and prediction tasks rather than explanation of relationships between variables. The tree structure represents recursive partitioning of the data to minimize residual deviance that is based on iteratively splitting the data into two subsets. 

The most basic type of tree-structure model is a decision tree which is a type of classification and regression tree (CART). A more elaborate version of a CART is called a Conditional Inference Tree (CIT). The difference between a CART and a CIT is that CITs use significance tests, e.g. the p-values, to select and split variables rather than some information measures like the Gini coefficient [@gries2021statistics].

Like random forests, inference trees are non-parametric and thus do not rely on distributional requirements (or at least on fewer).

## Advantages

Several advantages have been associated with using tree-based models:

1. Tree-structure models are very useful because they are **extremely flexible** as they can deal with different types of variables and provide a very good understanding of the structure in the data. 

2. Tree-structure models have been deemed particularly interesting for linguists because they can handle **moderate sample sizes** and many high-order interactions better then regression models. 

3. Tree-structure models are (supposedly) better at detecting **non-linear or non-monotonic relationships** between predictors and dependent variables. This also means that they are better at finding and displaying interaction sinvolving many predictors. 

4. Tree-structure models are **easy** to implement in R and do not require the model selection, validation, and diagnostics associated with regression models.

5. Tree-structure models can be used as **variable-selection** procedure which informs about which variables have any sort of significant relationship with the dependent variable and can thereby inform model fitting.

## Problems {-}

Despite these potential advantages, a word of warning is in order: @gries2021statistics admits that tree-based models can be very useful but there are some issues that but some serious short-comings of tree-structure models remain under-explored. For instance, 

1. Forest-models (Random Forests and Boruta) only inform about the **importance** of a variable but not if the variable is important as a main effect or as part of interactions (or both)! The importance only shows that there is some important connection between the predictor and the dependent variable. While partial dependence plots (see [here](https://christophm.github.io/interpretable-ml-book/pdp.html) for more information) offer a remedy for this shortcoming to a certain degree, regression models are still much better at dealing with this issue. 

2. Simple tree-structure models have been shown to fail in detecting the correct predictors if the variance is solely determined by a **single interaction** [@gries2021statistics, chapter 7.3]. This failure is caused by the fact that the predictor used in the first split of a tree is selected as the one with the strongest main effect [@boulesteix2015interaction, 344]. This issue can, however, be avoided by hard-coding the interactions as predictors plus using ensemble methods such as random forests rather than individual trees [see @gries2021statistics, chapter 7.3]. 

3. Another shortcoming is that tree-structure models partition the data (rather than "fitting a line" through the data which can lead to more **coarse-grained predictions** compared to regression models when dealing with numeric dependent variables [again, see @gries2021statistics, chapter 7.3]. 

4. @boulesteix2015interaction[341] state that high correlations between predictors can hinder the **detection of interactions** when using small data sets. However, regression do not fare better here as they are even more strongly affected by (multi-)collinearity [see @gries2021statistics, chapter 7.3].

5. Tree-structure models are bad a detecting interactions when the variables have **strong main effects** which is, unfortunately, common when dealing with linguistic data [@wrigt2016interac].

6. Tree-structure models cannot handle **factorial variables** with many levels (more than 53 levels) which is very common in linguistics where individual speakers or items are variables.

7. Forest-models (Random Forests and Boruta) have been deemed to be better at dealing with small data sets. However, this is only because the analysis is based on **permutations** of the original small data set. As such, forest-based models only appear to be better at handling small data sets because they "blow up" the data set rather than really being better at analyzing the original data.


Before we implement a conditional inference tree in R, we will have a look at how decision trees work. We will do this in more detail here as random forests and Boruta analyses are extensions of inference trees and are therefore based on the same concepts.
