[["index.html", "An introduction to conditional inference trees in R Introduction About me About the workshop Citation &amp; Session Info", " An introduction to conditional inference trees in R Martin Schweinberger Jan. 19, 2023 Introduction This website contains contains the materials for workshop An introduction to conditional inference trees in R offered Jan. 19, 2023, by Martin Schweinberger at the Rheinische Friedrich-Wilhelms-Universität Bonn. This workshop focuses on conditional inference trees and their implementation in R. The workshop uses materials provided by the Language Technology and Data Analysis Laboratory (LADAL). If you want a more detailed tutorial on tree-based methods going beyond what the workshop covers see this LADAL tutorial or you would like to know more about doing statistics and text analysis with R, please free to visit and explore the LADAL website. About me Quantitative corpus-linguist Lecturer in Applied linguistics at the University of Queensland (UQ) Associate Professor II at the Arctic University of Norway in Tromsø (UiT) Co-Director of the Language Technology and Data Analysis Laboratory (LADAL) at UQ Principal Data Science Advisor to the AcqVA Aurora Lab at UiT Projects: Australian Text Analytics Platform (ATAP) Language Data Commons of Australia (LDaCA) Studied Philosophy, English Philology and Psychology in Kassel and Galway PhD in Hamburg, Post docs (or similar positions) at the Linguistic Diversity in Urban Areas Excellence Cluster (LiMA), the FU Berlin, the Universities of Greifswald, Luneburg, and Kassel About the workshop Timeline | Table of Contents * 14:15 - 14:45Set up and Introduction * 14:45 - 15:00 What are tree-based models and When to use them * 15:00 - 15:15 What are pros and cons? * 15:15 - 15:40 Case study | Practice * 15:40 - 15:45 Outro During practice, we will use a Jupyter notebook and you can * sit back and follow * you can practice using the data provided by me * you can try and use your own data (but I cannot help you in modifying the code) Citation &amp; Session Info Schweinberger, Martin. 2023. An introduction to conditional inference trees in R. Bonn: Rheinische Friedrich-Wilhelms-Universität Bonn. url: https://MartinSchweinberger.github.io/TreesUBonn/index.html (Version 2023.01.19). @manual{schweinberger2023tree, author = {Schweinberger, Martin}, title = {An introduction to conditional inference trees in R}, note = {https://MartinSchweinberger.github.io/TreesUBonn/index.html}, year = {2023}, organization = &quot;Rheinische Friedrich-Wilhelms-Universität Bonn}, address = {Bonn}, edition = {2023.01.19} } Back to the workshop repo "],["basics-of-tree-based-models.html", "Basics of tree-based models Advantages Problems Classification And Regression Trees How Tree-Based Methods Work Splitting numeric, ordinal, and true categorical variables", " Basics of tree-based models Tree-structure models fall into the machine-learning rather than the inference statistics category as they are commonly used for classification and prediction tasks rather than explanation of relationships between variables. The tree structure represents recursive partitioning of the data to minimize residual deviance that is based on iteratively splitting the data into two subsets. The most basic type of tree-structure model is a decision tree which is a type of classification and regression tree (CART). A more elaborate version of a CART is called a Conditional Inference Tree (CIT). The difference between a CART and a CIT is that CITs use significance tests, e.g. the p-values, to select and split variables rather than some information measures like the Gini coefficient (Gries 2021). Like random forests, inference trees are non-parametric and thus do not rely on distributional requirements (or at least on fewer). Advantages Several advantages have been associated with using tree-based models: Tree-structure models are very useful because they are extremely flexible as they can deal with different types of variables and provide a very good understanding of the structure in the data. Tree-structure models have been deemed particularly interesting for linguists because they can handle moderate sample sizes and many high-order interactions better then regression models. Tree-structure models are (supposedly) better at detecting non-linear or non-monotonic relationships between predictors and dependent variables. This also means that they are better at finding and displaying interaction sinvolving many predictors. Tree-structure models are easy to implement in R and do not require the model selection, validation, and diagnostics associated with regression models. Tree-structure models can be used as variable-selection procedure which informs about which variables have any sort of significant relationship with the dependent variable and can thereby inform model fitting. Problems Despite these potential advantages, a word of warning is in order: Gries (2021) admits that tree-based models can be very useful but there are some issues that but some serious short-comings of tree-structure models remain under-explored. For instance, Forest-models (Random Forests and Boruta) only inform about the importance of a variable but not if the variable is important as a main effect or as part of interactions (or both)! The importance only shows that there is some important connection between the predictor and the dependent variable. While partial dependence plots (see here for more information) offer a remedy for this shortcoming to a certain degree, regression models are still much better at dealing with this issue. Simple tree-structure models have been shown to fail in detecting the correct predictors if the variance is solely determined by a single interaction (Gries 2021, chap. 7.3). This failure is caused by the fact that the predictor used in the first split of a tree is selected as the one with the strongest main effect (Boulesteix et al. 2015, 344). This issue can, however, be avoided by hard-coding the interactions as predictors plus using ensemble methods such as random forests rather than individual trees (see Gries 2021, chap. 7.3). Another shortcoming is that tree-structure models partition the data (rather than “fitting a line” through the data which can lead to more coarse-grained predictions compared to regression models when dealing with numeric dependent variables (again, see Gries 2021, chap. 7.3). Boulesteix et al. (2015, 341) state that high correlations between predictors can hinder the detection of interactions when using small data sets. However, regression do not fare better here as they are even more strongly affected by (multi-)collinearity (see Gries 2021, chap. 7.3). Tree-structure models are bad a detecting interactions when the variables have strong main effects which is, unfortunately, common when dealing with linguistic data (Wright, Ziegler, and König 2016). Tree-structure models cannot handle factorial variables with many levels (more than 53 levels) which is very common in linguistics where individual speakers or items are variables. Forest-models (Random Forests and Boruta) have been deemed to be better at dealing with small data sets. However, this is only because the analysis is based on permutations of the original small data set. As such, forest-based models only appear to be better at handling small data sets because they “blow up” the data set rather than really being better at analyzing the original data. Before we implement a conditional inference tree in R, we will have a look at how decision trees work. We will do this in more detail here as random forests and Boruta analyses are extensions of inference trees and are therefore based on the same concepts. Classification And Regression Trees Below is an example of a decision tree which shows what what response to expect - in this case whether a speaker uses discourse like or not. Decision trees, like all CARTs and CITs, answer a simple question, namely How do we best classify elements based on the given predictors?. The answer that decision trees provide is the classification of the elements based on the levels of the predictors. In simple decision trees, all predictors, even those that are not significant are included in the decision tree. The decision tree shows that the best (or most important) predictor for the use of discourse like is age as it is the highest node. Among young speakers, those with high status use like more compared with speakers of lower social status. Among old speakers, women use discourse like more than men. install.packages(&quot;tree&quot;) install.packages(&quot;dplyr&quot;) install.packages(&quot;grid&quot;) install.packages(&quot;flextable&quot;) install.packages(&quot;partykit&quot;) install.packages(&quot;ggparty&quot;) The yes and no at the bottom show if the speaker should be classified as a user of discourse like (yes or no). Each split can be read as true to the left and false to the right. So that, at the first split, if the person is between the ages of 15 and 40, we need to follow the branch to the left while we need to follow to the right if the person is not 15 to 40. Before going through how this conditional decision tree is generated, let us first go over some basic concepts. The top of the decision tree is called root or root node, the categories at the end of branches are called leaves or leaf nodes. Nodes that are in-between the root and leaves are called internal nodes or just nodes. The root node has only arrows or lines pointing away from it, internal nodes have lines going to and from them, while leaf nodes only have lines pointing towards them. How to prune and evaluate the accuracy of decision trees is not shown here. If you are interested in this, please check out chapter 7 of Gries (2021) which is a highly recommendable resource that provide a lot of additional information about decision trees and CARTs. We will now focus on how to implement tree-based models in R and continue with getting a more detaled understanding og how tree-based methods work. How Tree-Based Methods Work Let us now go over the process by which the decision tree above is generated. In our example, we want to predict whether a person makes use of discourse like given their age, gender, and social status. Let us now go over the process by which the decision tree above is generated. In our example, we want to predict whether a person makes use of discourse like given their age, gender, and social status. In a first step, we load and inspect the data that we will use in this tutorial. As tree-based models require either numeric or factorized data, we factorize the “character” variables in our data. # load data citdata &lt;- read.delim(&quot;https://slcladal.github.io/data/treedata.txt&quot;, header = T, sep = &quot;\\t&quot;) %&gt;% dplyr::mutate_if(is.character, factor) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; border-color: transparent; caption-side: top; } .tabwid-caption-bottom table{ caption-side: bottom; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td, .tabwid th { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .katex-display { margin: 0 0 !important; } .cl-0fdd766a{table-layout:auto;width:75%;}.cl-0fd2cb20{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0fd2cb2a{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-0fd71572{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-0fd732c8{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0fd732d2{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0fd732dc{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0fd732dd{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0fd732de{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0fd732e6{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0fd732f0{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0fd732f1{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0fd732fa{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0fd732fb{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0fd73304{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-0fd73305{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 1: First 10 rows of the citdata data AgeGenderStatusLikeUser15-40femalehighno15-40femalehighno15-40malehighno41-80femalelowyes41-80malehighno41-80malelowno41-80femalelowyes15-40malehighno41-80malelowno41-80malelowno The data now consists of factors which two levels each. The first step in generating a decision tree consists in determining, what the root of the decision tree should be. This means that we have to determine which of the variables represents the root node. In order to do so, we tabulate for each variable level, how many speakers of that level have used discourse like (LikeUsers) and how many have not used discourse like (NonLikeUsers). # tabulate data table(citdata$LikeUser, citdata$Gender) ## ## female male ## no 43 75 ## yes 91 42 table(citdata$LikeUser, citdata$Age) ## ## 15-40 41-80 ## no 34 84 ## yes 92 41 table(citdata$LikeUser, citdata$Status) ## ## high low ## no 33 85 ## yes 73 60 None of the predictors is perfect (the predictors are therefore referred to as impure). To determine which variable is the root, we will calculate the degree of “impurity” for each variable - the variable which has the lowest impurity value will be the root. The most common measure of impurity in the context of conditional inference trees is called Gini (an alternative that is common when generating regression trees is the deviance). The Gini value or gini index was introduced by Corrado Gini as a measure for income inequality. In our case we seek to maximize inequality of distributions of leave nodes which is why the gini index is useful for tree based models. For each level we apply the following equation to determine the gini impurity value: \\[\\begin{equation} G_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2} \\end{equation}\\] For the node for men, this would mean the following: \\[\\begin{equation} G_{men} = 1-(\\frac{42} {42+75})^{2} - (\\frac{75} {42+75})^{2} = 0.4602235 \\end{equation}\\] For women, we calculate G or Gini as follows: \\[\\begin{equation} G_{women} = 1-(\\frac{91} {91+43})^{2} - (\\frac{43} {91+43})^{2} = 0.4358432 \\end{equation}\\] To calculate the Gini value of Gender, we need to calculate the weighted average leaf node impurity (weighted because the number of speakers is different in each group). We calculate the weighted average leaf node impurity using the equation below. \\[\\begin{equation} G_{Gender} = \\frac{N_{men}} {N_{Total}} \\times G_{men} + \\frac{N_{women}} {N_{Total}} \\times G_{women} \\end{equation}\\] \\[\\begin{equation} G_{Gender} = \\frac{159} {303} \\times 0.4602235 + \\frac{144} {303} \\times 0.4358432 = 0.4611915 \\end{equation}\\] We will now perform the gini-calculation for gender (see below). # calculate Gini for men gini_men &lt;- 1-(42/(42+75))^2 - (75/(42+75))^2 # calculate Gini for women gini_women &lt;- 1-(91/(91+43))^2 - (43/(91+43))^2 # calculate weighted average of Gini for Gender gini_gender &lt;- 42/(42+75)* gini_men + 91/(91+43) * gini_women gini_gender ## [1] 0.4611915 The gini for gender is 0.4612. In a next step, we revisit the age distribution and we continue to calculate the gini value for age. # calculate Gini for age groups gini_young &lt;- 1-(92/(92+34))^2 - (34/(92+34))^2 # Gini: young gini_old &lt;- 1-(41/(41+84))^2 - (84/(41+84))^2 # Gini: old # calculate weighted average of Gini for Age gini_age &lt;- 92/(92+34)* gini_young + 41/(41+84) * gini_old gini_age ## [1] 0.4323148 The gini for age is .4323 and we continue by revisiting the status distribution and we continue to calculate the gini value for status. gini_high &lt;- 1-(73/(33+73))^2 - (33/(33+73))^2 # Gini: high gini_low &lt;- 1-(60/(60+85))^2 - (85/(60+85))^2 # Gini: low # calculate weighted average of Gini for Status gini_status &lt;- 73/(33+73)* gini_high + 60/(60+85) * gini_low gini_status ## [1] 0.4960521 The gini for status is .4961 and we can now compare the gini values for age, gender, and status. # compare age, gender, and status ginis gini_age; gini_gender; gini_status ## [1] 0.4323148 ## [1] 0.4611915 ## [1] 0.4960521 Since age has the lowest gini (impurity) value, our first split is by age and age, thus, represents our root node. Our manually calculated conditional inference tree right now looks as below. In a next step, we need to find out which of the remaining variables best separates the speakers who use discourse like from those that do not under the first node. In order to do so, we calculate the Gini values for Gender and SocialStatus for the 15-40 node. We thus move on and test if and how to split this branch. # 5TH NODE # split data according to first split (only young data) young &lt;- citdata %&gt;% dplyr::filter(Age == &quot;15-40&quot;) # inspect distribution tbyounggender &lt;- table(young$LikeUser, young$Gender) tbyounggender ## ## female male ## no 17 17 ## yes 58 34 # calculate Gini for Gender # calculate Gini for men gini_youngmen &lt;- 1-(tbyounggender[2,2]/sum(tbyounggender[,2]))^2 - (tbyounggender[1,2]/sum(tbyounggender[,2]))^2 # calculate Gini for women gini_youngwomen &lt;- 1-(tbyounggender[2,1]/sum(tbyounggender[,1]))^2 - (tbyounggender[1,1]/sum(tbyounggender[,1]))^2 # # calculate weighted average of Gini for Gender gini_younggender &lt;- sum(tbyounggender[,2])/sum(tbyounggender)* gini_youngmen + sum(tbyounggender[,1])/sum(tbyounggender) * gini_youngwomen gini_younggender ## [1] 0.3885714 The gini value for gender among young speakers is 0.3886. We continue by inspecting the status distribution. # calculate Gini for Status # inspect distribution tbyoungstatus &lt;- table(young$LikeUser, young$Status) tbyoungstatus ## ## high low ## no 11 23 ## yes 57 35 We now calculate the gini value for status. # calculate Gini for status # calculate Gini for low gini_younglow &lt;- 1-(tbyoungstatus[2,2]/sum(tbyoungstatus[,2]))^2 - (tbyoungstatus[1,2]/sum(tbyoungstatus[,2]))^2 # calculate Gini for high gini_younghigh &lt;- 1-(tbyoungstatus[2,1]/sum(tbyoungstatus[,1]))^2 - (tbyoungstatus[1,1]/sum(tbyoungstatus[,1]))^2 # # calculate weighted average of Gini for status gini_youngstatus &lt;- sum(tbyoungstatus[,2])/sum(tbyoungstatus)* gini_younglow + sum(tbyoungstatus[,1])/sum(tbyoungstatus) * gini_younghigh gini_youngstatus ## [1] 0.3666651 Since the gini value for status (0.3667) is lower than the gini value for gender (0.3886), we split by status. We would continue to calculate the gini values and always split at the lowest gini levels until we reach a leaf node. Then, we would continue doing the same for the remaining branches until the entire data is binned into different leaf nodes. In addition to plotting the decision tree, we can also check its accuracy. To do so, we predict the use of like based on the decision tree and compare them to the observed uses of like. Then we use the confusionMatrix function from the caret package to get an overview of the accuracy statistics. dtreeprediction &lt;- as.factor(ifelse(predict(dtree)[,2] &gt; .5, &quot;yes&quot;, &quot;no&quot;)) confusionMatrix(dtreeprediction, citdata$LikeUser) The conditional inference tree has an accuracy of 72.9 percent which is significantly better than the base-line accuracy of 53.0 percent (No Information Rate \\(*\\) 100). To understand what the other statistics refer to and how they are calculated, run the command ?confusionMatrix. Splitting numeric, ordinal, and true categorical variables While it is rather straight forward to calculate the Gini values for categorical variables, it may not seem quite as apparent how to calculate splits for numeric or ordinal variables. To illustrate how the algorithm works on such variables, consider the example data set shown below. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; border-color: transparent; caption-side: top; } .tabwid-caption-bottom table{ caption-side: bottom; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td, .tabwid th { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .katex-display { margin: 0 0 !important; } .cl-10ad1032{table-layout:auto;width:75%;}.cl-10a05d1a{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-10a05d24{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-10a50ab8{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-10a50ac2{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-10a52e08{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10a52e12{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10a52e13{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10a52e1c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10a52e1d{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10a52e26{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10a52e27{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10a52e28{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 2: First 10 rows of the citdata2 data. AgeLikeUser15yes37no63no42yes22yes27yes In a first step, we order the numeric variable so that we arrive at the following table. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; border-color: transparent; caption-side: top; } .tabwid-caption-bottom table{ caption-side: bottom; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td, .tabwid th { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .katex-display { margin: 0 0 !important; } .cl-10cfca8c{table-layout:auto;width:75%;}.cl-10c54756{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-10c54760{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-10c99842{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-10c99856{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-10c9b44e{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10c9b458{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10c9b459{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10c9b462{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10c9b46c{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10c9b46d{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10c9b46e{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10c9b476{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 3: First 10 rows of the citdata2 data arranged by age. AgeLikeUser15yes22yes27yes37no42yes63no Next, we calculate the means for each level of “Age”. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; border-color: transparent; caption-side: top; } .tabwid-caption-bottom table{ caption-side: bottom; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td, .tabwid th { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .katex-display { margin: 0 0 !important; } .cl-10ed3aa4{table-layout:auto;width:75%;}.cl-10e2a4f4{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-10e2a4fe{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-10e6a996{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-10e6a9aa{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-10e6c502{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10e6c50c{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10e6c50d{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10e6c50e{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10e6c516{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10e6c517{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10e6c520{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-10e6c521{background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 4: First 10 rows of the citdata3 data arranged by age. AgeLikeUser15.0yes18.522.0yes24.527.0yes32.037.0no39.542.0yes52.5 Now, we calculate the Gini values for each average level of age. How this is done is shown below for the first split. \\[\\begin{equation} G_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2} \\end{equation}\\] For an age smaller than 18.5 this would mean: \\[\\begin{equation} G_{youngerthan18.5} = 1-(\\frac{1} {1+0})^{2} - (\\frac{0} {1+0})^{2} = 0.0 \\end{equation}\\] For an age greater than 18.5, we calculate G or Gini as follows: \\[\\begin{equation} G_{olerthan18.5} = 1-(\\frac{2} {2+3})^{2} - (\\frac{3} {2+3})^{2} = 0.48 \\end{equation}\\] Now, we calculate the Gini for that split as we have done above. \\[\\begin{equation} G_{split18.5} = \\frac{N_{youngerthan18.5}} {N_{Total}} \\times G_{youngerthan18.5} + \\frac{N_{olderthan18.5}} {N_{Total}} \\times G_{olderthan18.5} G_{split18.5} = \\frac{1} {6} \\times 0.0 + \\frac{5} {6} \\times 0.48 = 0.4 \\end{equation}\\] We then have to calculate the gini values for all possible age splits which yields the following results: # 18.5 1-(1/(1+0))^2 - (0/(1+0))^2 1-(2/(2+3))^2 - (3/(2+3))^2 1/6 * 0.0 + 5/6 * 0.48 # 24.4 1-(2/(2+0))^2 - (0/(2+0))^2 1-(3/(3+1))^2 - (2/(3+1))^2 2/6 * 0.0 + 4/6 * 0.1875 # 32 1-(3/(3+0))^2 - (0/(3+0))^2 1-(1/(1+2))^2 - (2/(1+2))^2 3/6 * 0.0 + 3/6 * 0.4444444 # 39.5 1-(3/(3+1))^2 - (1/(3+1))^2 1-(1/(1+1))^2 - (1/(1+1))^2 4/6 * 0.375 + 2/6 * 0.5 # 52.5 1-(4/(4+1))^2 - (1/(4+1))^2 1-(0/(0+1))^2 - (1/(0+1))^2 5/6 * 0.32 + 1/6 * 0.0 .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; border-color: transparent; caption-side: top; } .tabwid-caption-bottom table{ caption-side: bottom; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td, .tabwid th { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .katex-display { margin: 0 0 !important; } .cl-110f8a5a{table-layout:auto;width:75%;}.cl-11051a3e{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-11051a48{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-11093dc6{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1109587e{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-11095888{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-11095889{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-11095892{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-11095893{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-11095894{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1109589c{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1109589d{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 5: First 10 rows of the citdata3 data with Gini coefficients. AgeSplitGini18.50.40024.50.50032.00.44439.50.41052.50.267 The split at 52.5 years of age has the lowest Gini value. Accordingly, we would split the data between speakers who are younger than 52.5 and speakers who are older than 52.5 years of age. The lowest Gini value for any age split would also be the Gini value that would be compared to other variables. The same procedure that we have used to determine potential splits for a numeric variable would apply to an ordinal variable with only two differences: The Gini values are calculated for the actual levels and not the means between variable levels. The Gini value is nor calculated for the lowest and highest level as the calculation of the Gini values is impossible for extreme values. Extreme levels can, therefore, not serve as a potential split location. When dealing with categorical variables with more than two levels, the situation is slightly more complex as we would also have to calculate the Gini values for combinations of variable levels. While the calculations are, in principle, analogous to the ones performed for binary of nominal categorical variables, we would also have to check if combinations would lead to improved splits. For instance, imagine we have a variable with categories A, B, and C. In such cases we would not only have to calculate the Gini scores for A, B, and C but also for A plus B, A plus C, and B plus C. Note that we ignore the combination A plus B plus C as this combination would include all other potential combinations. Back to the workshop repo References "],["conditional-inference-trees.html", "Conditional Inference Trees 0.1 Example 1: disourse like 0.2 Example 2: Prepositions Prettifying your CIT tree Extensions of Conditional Inference Trees", " Conditional Inference Trees Conditional Inference Trees (CITs) are much better at determining the true effect of a predictor, i.e. the effect of a predictor if all other effects are simultaneously considered. In contrast to CARTs, CITs use p-values to determine splits in the data. Below is a conditional inference tree which shows how and what factors contribute to the use of discourse like. In conditional inference trees predictors are only included if the predictor is significant (i.e. if these predictors are necessary). 0.1 Example 1: disourse like In a first step, we load the data (in this case, the data is stored online and made available via the LADAL github repo). citdata &lt;- read.delim(&quot;https://slcladal.github.io/data/treedata.txt&quot;, header = T, sep = &quot;\\t&quot;) # inspect the data (head shows the first 6 lines) head(citdata) ## Age Gender Status LikeUser ## 1 15-40 female high no ## 2 15-40 female high no ## 3 15-40 male high no ## 4 41-80 female low yes ## 5 41-80 male high no ## 6 41-80 male low no We can also inspect the structure of the data using the str() function as shown below. # inspect the data (str shows the structure of the data) str(citdata) ## &#39;data.frame&#39;: 251 obs. of 4 variables: ## $ Age : chr &quot;15-40&quot; &quot;15-40&quot; &quot;15-40&quot; &quot;41-80&quot; ... ## $ Gender : chr &quot;female&quot; &quot;female&quot; &quot;male&quot; &quot;female&quot; ... ## $ Status : chr &quot;high&quot; &quot;high&quot; &quot;high&quot; &quot;low&quot; ... ## $ LikeUser: chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ... We can see that all variables are character variables (indicated by the chr next to the variable name). However, CITs require factors (not character strings) and we thus need to convert the variables into factors (which we can do using mutate_if(is.character, factor)) # convert character strings to factors citdata &lt;- citdata %&gt;% dplyr::mutate_if(is.character, factor) # inspect the data (str shows the structure of the data) str(citdata) ## &#39;data.frame&#39;: 251 obs. of 4 variables: ## $ Age : Factor w/ 2 levels &quot;15-40&quot;,&quot;41-80&quot;: 1 1 1 2 2 2 2 1 2 2 ... ## $ Gender : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 1 2 1 2 2 1 2 2 2 ... ## $ Status : Factor w/ 2 levels &quot;high&quot;,&quot;low&quot;: 1 1 1 2 1 2 2 1 2 2 ... ## $ LikeUser: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 2 1 1 2 1 1 1 ... Now we can implement the CIT. We start by defining a so-called control which allows us to model that we are running several test in one go. Setting a control in this case means that we lower the significance levels so that the overall significance level (of all test combined is still at \\(\\alpha\\) = .05 (this prevents errors in our analysis called false positives). # apply bonferroni correction (1 minus alpha multiplied by n of predictors) control = ctree_control(mincriterion = 1-(.05*ncol(citdata)-1)) After setting the control, we implement the CIT by using the ctree function from the partykit package. This function requires the formula (the dependent variable ~ and the independent variables) and the specification of the data. Then, we plot the resulting CIT using the plot function. # create initial conditional inference tree model citd.ctree &lt;- partykit::ctree(LikeUser ~ Age + Gender + Status, data = citdata) plot(citd.ctree, gp = gpar(fontsize = 8)) # plot final ctree 0.2 Example 2: Prepositions We now proceed with a different example that uses a numeric dependent variable. In this example, we want to see what factors impact the use of prepositions across time in historical texts. The analysis is based on data extracted from the Penn Corpora of Historical English (see http://www.ling.upenn.edu/hist-corpora/), that consists of 603 texts written between 1125 and 1900. Be start again by loading the data. pvd &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/pvd.rda&quot;, &quot;rb&quot;)) %&gt;% dplyr::mutate_if(is.character, factor) %&gt;% # remove columns we do not need dplyr::select(-Date, -Genre, -Text) # inspect head(pvd); str(pvd) ## Prepositions Region GenreRedux DateRedux ## 1 166.01 North NonFiction 1700-1799 ## 2 139.86 North NonFiction 1700-1799 ## 3 130.78 North Conversational 1800-1913 ## 4 151.29 North NonFiction 1800-1913 ## 5 145.72 North NonFiction 1700-1799 ## 6 120.77 North NonFiction 1800-1913 ## &#39;data.frame&#39;: 537 obs. of 4 variables: ## $ Prepositions: num 166 140 131 151 146 ... ## $ Region : Factor w/ 2 levels &quot;North&quot;,&quot;South&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ GenreRedux : Factor w/ 5 levels &quot;Conversational&quot;,..: 4 4 1 4 4 4 1 4 4 1 ... ## $ DateRedux : Factor w/ 5 levels &quot;1150-1499&quot;,&quot;1500-1599&quot;,..: 4 4 5 5 4 5 5 5 4 4 ... Next, we set the control just like we did before. # apply bonferroni correction (1 minus alpha multiplied by n of predictors) control = ctree_control(mincriterion = 1-(.05*ncol(pvd)-1)) Now, we implement the CIT and plot the results. # create initial conditional inference tree model citd.ctree2 &lt;- partykit::ctree(Prepositions ~ DateRedux + Region + GenreRedux, data = pvd) plot(citd.ctree2, gp = gpar(fontsize = 8)) # plot final ctree Prettifying your CIT tree The easiest and most common way to visualize CITs is to simply use the plot function from base R. However, using this function does not allow to adapt and customize the visualization except for some very basic parameters. The ggparty function allows to use the ggplot syntax to customize CITs which allows more adjustments and is more aesthetically pleasing. To generate this customized CIT, we activate the ggparty package and extract the significant p-values from the CIT object. We then plot the CIT and define the nodes, edges, and text elements as shown below. # extract p-values pvals &lt;- unlist(nodeapply(citd.ctree, ids = nodeids(citd.ctree), function(n) info_node(n)$p.value)) pvals &lt;- pvals[pvals &lt;.05] # plotting ggparty(citd.ctree) + geom_edge() + geom_edge_label() + geom_node_label(line_list = list(aes(label = splitvar), aes(label = paste0(&quot;N=&quot;, nodesize, &quot;, p&quot;, ifelse(pvals &lt; .001, &quot;&lt;.001&quot;, paste0(&quot;=&quot;, round(pvals, 3)))), size = 10)), line_gpar = list(list(size = 13), list(size = 10)), ids = &quot;inner&quot;) + geom_node_label(aes(label = paste0(&quot;Node &quot;, id, &quot;, N = &quot;, nodesize)), ids = &quot;terminal&quot;, nudge_y = -0.0, nudge_x = 0.01) + geom_node_plot(gglist = list( geom_bar(aes(x = &quot;&quot;, fill = LikeUser), position = position_fill(), color = &quot;black&quot;), theme_minimal(), scale_fill_manual(values = c(&quot;gray50&quot;, &quot;gray80&quot;), guide = FALSE), scale_y_continuous(breaks = c(0, 1)), xlab(&quot;&quot;), ylab(&quot;Probability&quot;), geom_text(aes(x = &quot;&quot;, group = LikeUser, label = stat(count)), stat = &quot;count&quot;, position = position_fill(), vjust = 1.1)), shared_axis_labels = TRUE) We can also use position_dodge (instead of position_fill) to display frequencies rather than probabilities as shown below. # plotting ggparty(citd.ctree) + geom_edge() + geom_edge_label() + geom_node_label(line_list = list(aes(label = splitvar), aes(label = paste0(&quot;N=&quot;, nodesize, &quot;, p&quot;, ifelse(pvals &lt; .001, &quot;&lt;.001&quot;, paste0(&quot;=&quot;, round(pvals, 3)))), size = 10)), line_gpar = list(list(size = 13), list(size = 10)), ids = &quot;inner&quot;) + geom_node_label(aes(label = paste0(&quot;Node &quot;, id, &quot;, N = &quot;, nodesize)), ids = &quot;terminal&quot;, nudge_y = 0.01, nudge_x = 0.01) + geom_node_plot(gglist = list( geom_bar(aes(x = &quot;&quot;, fill = LikeUser), position = position_dodge(), color = &quot;black&quot;), theme_minimal(), theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()), scale_fill_manual(values = c(&quot;gray50&quot;, &quot;gray80&quot;), guide = FALSE), scale_y_continuous(breaks = seq(0, 100, 20), limits = c(0, 100)), xlab(&quot;&quot;), ylab(&quot;Frequency&quot;), geom_text(aes(x = &quot;&quot;, group = LikeUser, label = stat(count)), stat = &quot;count&quot;, position = position_dodge(0.9), vjust = -0.7)), shared_axis_labels = TRUE) Extensions of Conditional Inference Trees An extension which remedies this problem is to use a so-called ensemble method which grows many varied trees. The most common ensemble method is called a Random Forest Analysis and, unfortunately, we cannot deal with random forests here (but if you are interested, check out this tutorial). Back to the workshop repo "],["practice.html", "Practice", " Practice To see how to implement a conditional inference tree, we will use a Jupyter notebook made available to everyone on Google Colab. Please just follow the link provided below to open the Jupyter notebook. Click this link to open an interactive version of this tutorial on Binder.org. If the notebook on Binder does not work, please click this link to open an interactive version of this tutorial on Google Colab. This interactive Jupyter notebook allows you to execute code yourself and you can also change and edit the notebook, e.g. you can change code and upload your own data. Back to the workshop repo "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
